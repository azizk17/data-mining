{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #numpy for arithmatic operation\n",
    "import pandas as pd # pandas for dataset operations\n",
    "import matplotlib.pyplot as plt # visualization library\n",
    "%matplotlib inline              \n",
    "import seaborn as sns           #scientific visualization\n",
    "from sklearn.model_selection import train_test_split # dataset split for training and test\n",
    "from sklearn.tree import DecisionTreeClassifier # Decision tree model library\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNN model Library\n",
    "from sklearn.ensemble import RandomForestClassifier # random forest model library\n",
    "from sklearn.metrics import accuracy_score,classification_report, confusion_matrix #performance evaluation metrics\n",
    "from sklearn.metrics import f1_score #performance evaluation metrics for F1 score\n",
    "# Feature Dimention Reduction by PCA \n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Downloading the datasets"
   ]
  },
  {
   "source": [
    "## 1. Importing the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('dataset/data/covertype_csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tDescribe the dataset and the classification task, more information about the dataset can be found in UCI repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contain 581012 and 55 columns. The dataset is used for classification tasks. The dataset does not contain any missing values.This dataset is multivariate. The dataset have cartographic variables that consist of categorical and numeric data types values. The class(covertype) is the target variable that is multiclassification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Data Exploration:\n"
   ]
  },
  {
   "source": [
    "## 3. Display the number of instances.\n",
    "## 4.\tDisplay the number of attributes.\n",
    "## 5.    Display the number of classes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The data has {df.shape[0]} records, {df.shape[1]} attributes, and {df['class'].unique()} clasess\")"
   ]
  },
  {
   "source": [
    "##  6.\tFor each class label, display the code of the class label and the name of that class.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('class').size()"
   ]
  },
  {
   "source": [
    "##  7.\tSummarise the class distribution using a suitable graph.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The instances distribution of target variable can be visualized through countplot\n",
    "# sns.countplot(df['class'])\n",
    "sns.distplot(df['class'])"
   ]
  },
  {
   "source": [
    "##  8. Display a statistical summary for all the attributes.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   C. Data Preprocessing\n"
   ]
  },
  {
   "source": [
    "## 9. Check whether the selected dataset has any data quality issues and choose suitable strategies to deal with any issue (if exists)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we check the quality of data through histogram that tell about the distribution of the dataset. The overall distributon of dataset\n",
    "#standard normal distributon. however some attributes contain abnormal distribution. \n",
    "df.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)"
   ]
  },
  {
   "source": [
    "##  10.\tConvert the multiclass classification problem into a binary classification problem."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert classes that has a value grater than 4 to 1\n",
    "df.loc[df['class'] <= 3] = 0 \n",
    "# convert classes that has a value less than 3 to 0\n",
    "df.loc[df['class'] >= 4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just making sure everthing works as expected\n",
    "df.groupby('class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "X=df.iloc[:, :-1]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target variable\n",
    "y=df.iloc[:, -1:]\n",
    "y.head()"
   ]
  },
  {
   "source": [
    "## 11. Use a features selection technique to select those features in your data that contribute most to the prediction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*Take a partial of the data, since it is too big which cause the machine to hang* \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.001, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca model call\n",
    "pca = PCA(n_components=8, random_state=42)\n",
    "# pca model fit on featues\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the feautes\n",
    "X = pca.transform(X)"
   ]
  },
  {
   "source": [
    "## 12. Divide your dataset into training, validation and testing datasets.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test size=30%\n",
    "# training size 50%\n",
    "# validation size=20%\n",
    "random_seed=123\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, \n",
    "                                                    train_size=0.5, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y_test)\n",
    "    "
   ]
  },
  {
   "source": [
    "# D. classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.\tBuild classification models.\n",
    "\n"
   ]
  },
  {
   "source": [
    "### a.\tUse three different learning algorithms to generate three classification models. You should choose one learning algorithm from each of the following categories:\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Decision_Tree', 'Nearest_Neighbors', 'Random_Forest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [ DecisionTreeClassifier(random_state=24), KNeighborsClassifier(3), RandomForestClassifier(random_state=24) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "predictions_test = []\n",
    "predictions_vald = []\n",
    "\n",
    "for name, clf in zip(model_names, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions_test.append(clf.predict(X_test))\n",
    "    predictions_vald.append(clf.predict(X_val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accucarcy\n",
    "accuracy_test = []\n",
    "\n",
    "for name, pred in zip(model_names, predictions_test):\n",
    "    score = accuracy_score(y_test, pred)\n",
    "    accuracy.append(score)\n",
    "    print(f'{name} test accuracy is {score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_vald = []\n",
    "    \n",
    "for name, pred in zip(model_names, predictions_vald):\n",
    "    score = accuracy_score(y_test, pred)\n",
    "    accuracy.append(score)\n",
    "    print(f'{name} validation accuracy is {score}')"
   ]
  },
  {
   "source": [
    "#### i.\tDecision Tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(random_state=24) # using the random state for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree model training\n",
    "dtree = dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree model prediction on test data\n",
    "dtree_pred = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our decision tree model\n",
    "print(classification_report(y_test, dtree_pred))\n"
   ]
  },
  {
   "source": [
    "#### ii. Nearest Neighbor Classifier, ~~Naive Bayes Classifier, Support Vector Machine~~"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN  model training\n",
    "knn =knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN  model prediction on test data\n",
    "knn_prediction = knn.predict(X_test)"
   ]
  },
  {
   "source": [
    "#### iii.   ~~Bagging, Boosting,~~ Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forest model \n",
    "forest= RandomForestClassifier(random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest model training\n",
    "forest=forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest model prediction on test\n",
    "forest_prediction= forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.\tFor each classification model:\n"
   ]
  },
  {
   "source": [
    "a.\tTry to find the most accurate classifier (avoid overfitting).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # then predict on the test set\n",
    "dt_accuracy= accuracy_score(y_test, dtree_pred) \n",
    "rf_accuracy= accuracy_score(y_test, forest_prediction) \n",
    "knn_accuracy= accuracy_score(y_test, knn_prediction) \n",
    "\n",
    "print(dt_accuracy)\n",
    "print(rf_accuracy)\n",
    "print(knn_accuracy)"
   ]
  },
  {
   "source": [
    "# E. Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.\tEvaluate your classification models on the validation and the testing datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model:  Decision_Tree\nConfusion matrix for testing dataset:  [0 0 0 ... 0 0 0]\nConfusion matrix for validation dataset:  [0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for name, pred in zip(model_names, predictions_test):\n",
    "    print(\"Model: \", name)\n",
    "    print(\"classification report testing dataset: \", classification_report(y_test, pred))\n",
    "    print(\"Confusion matrix for testing dataset: \", confusion_matrix(y_test, pred ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pred in zip(model_names, predictions_vald):\n",
    "    vald_pred = clf.predict(X_val)\n",
    "    print(\"Model: \", name)\n",
    "    print(\"classification report validation dataset: \", classification_report(y_vald, pred))    \n",
    "    print(\"Confusion matrix for validation dataset: \",  confusion_matrix(y_vald, pred )"
   ]
  },
  {
   "source": [
    "### a.\tFor each classification model, print out a confusion matrix for the validation and testing datasets.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate your classification models on the validation and the testing datasets.\n",
    "dtree_test_pred = dtree.predict(X_test)\n",
    "dtree_vald_pred = dtree.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a For each classification model, print out a confusion matrix for the validation and testing datasets.\n",
    "cm1= confusion_matrix(y_test, dtree_test_pred)\n",
    "cm2= confusion_matrix(y_test, dtree_vald_pred)"
   ]
  },
  {
   "source": [
    "### b. Use the following evaluation measures to evaluate the performance of the generated classification models:\n",
    "###    *i. Accuracy\tii. Error rate\tiii. F -measure*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- #### *Testing Dataset*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##### i.Accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy=accuracy_score(y_test, y_pred)\n",
    "print(model_accuracy)"
   ]
  },
  {
   "source": [
    "##### ii. Error rate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate = 1 - model_accuracy\n",
    "print(error_rate)"
   ]
  },
  {
   "source": [
    "##### iii. F -measure"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('F1 score:', f1_score(y_test, y_pred))"
   ]
  },
  {
   "source": [
    "- #### Validation Dataset\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##### i. accuracy\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_accuracy=accuracy_score(y_test, prediction)\n",
    "print('Accuracy:',classifier_accuracy)"
   ]
  },
  {
   "source": [
    "##### ii. error rate\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate_val=1-classifier_accuracy\n",
    "print('error_rate:',error_rate_val)"
   ]
  },
  {
   "source": [
    "##### iii. F-measure"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('F1 score:', f1_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Compare between the performances of all the classification models using suitable chart (The type of chart should be different from the type of the chart that is used in the data exploration stage).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking model result\n",
    "dt_accuracy\n",
    "rf_accuracy\n",
    "knn_accuracy\n",
    "print(dt_accuracy)\n",
    "print(rf_accuracy)\n",
    "print(knn_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=['dt_accuracy', 'rf_accuracy', 'knn_accuracy']\n",
    "accuracy=[0.97, 1.0, 0.95]\n",
    "plt.bar(models, accuracy)\n",
    "plt.title('Accuracy of Models')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "f9a665eed82786d1db64405b5cfd0f8037f4571b806793cb1072a84e044fc7ff"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}